{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# os.getcwd()\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Memory\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory # save all conversation\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "# memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "# memory.load_memory_variables({})\n",
    "memory = ConversationBufferMemory(return_messages=True) # 챗 형태 출력\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    x = memory.load_memory_variables({})\n",
    "    return {\"history\": x[\"history\"]}\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | model\n",
    "\n",
    "inputs = {\"message\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ") # window_size 4개의 대화만 저장하고 나머지는 삭제\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(5, 5)\n",
    "\n",
    "memory.load_memory_variables({}) # 1 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# save only summary of conversation\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.3 summary + buffer\n",
    "# 메모리의 메시지 수를 저장, 최근 정보만 추적\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150, # 최대 토큰 수\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is England from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"How far is England from Argentina?\", \"I don't know! Super far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is England from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is England from America?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"How far is England from America?\", \"I don't know! Super far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Knowledge graph\n",
    "# 대화를 지식 그래프 형태로 저장\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea.')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specific message\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea. Nicolas likes kimchi.')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 수 저장 - Conversation Token Buffer\n",
    "# entities를 저장하는 방식도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 chain에 메모리 적용\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain # off-the-shelf chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # max_token_limit=80,\n",
    "    memory_key=\"chat_history\", # 메모리 키 지정, 템플릿 내 변수명 동일\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    # prompt=PromptTemplate.from_template({\"question\"}),\n",
    "    verbose=True, # Prompt 로그를 볼 수 있음\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")\n",
    "memory.load_memory_variables({}) # 요약 내용 들어가 있음 ~ 프롬프트로 써야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 메모리 저장 방식 결정\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True, # 메시지 클래스로 바꾸어 줌\n",
    ")\n",
    "\n",
    "# 메시지 공간을 지정함\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 LCEL로 표현하는 방법 (custom chain)\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# chain = prompt | llm\n",
    "# chain.invoke\n",
    "\n",
    "# load_memory 먼저 불러온 이후 history에 저장\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"My name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8 메모리 저장하고 다시 프롬프트에 넣어서 활용함\n",
    "# LLM cahin, prompt template, chat prompt template, manual way\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# 메모리 불러오기\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    # result = chain.invoke({\"question\": question, \"history\": load_memory()})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
